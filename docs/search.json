[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep VAR",
    "section": "",
    "text": "On this website you will find tutorials and high-level documentation for deepvars."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "rnn_torch.html#overview",
    "href": "rnn_torch.html#overview",
    "title": "RNN in Torch",
    "section": "Overview",
    "text": "Overview\nThis tutorial demonstrates how the Recurrent Neural Network underlying Deep VAR is set up using torch for R. It heavily draws on ideas and code presented in this great tutorial from the RStudio AI blog."
  },
  {
    "objectID": "rnn_torch.html#data-input",
    "href": "rnn_torch.html#data-input",
    "title": "RNN in Torch",
    "section": "Data input",
    "text": "Data input\n\ntrain_val_test_split <- function(data, train_size=0.8) {\n  N <- nrow(data)\n  end_train <- round(train_size * N)\n  end_val <- end_train + round((N - end_train)/2)\n  data_train <- data[1:end_train,] |> as.matrix()\n  data_val <- data[(end_train+1):end_val,] |> as.matrix()\n  data_test <- data[(end_val+1):N,] |> as.matrix()\n  return(list(train=data_train, val=data_val, test=data_test))\n}\n\n\nlibrary(deepvars)\nlibrary(data.table)\ndata(\"canada\")\ndt <- data.table(canada)\nvar_cols = colnames(dt)[2:ncol(dt)]\ndt[,(var_cols) := lapply(.SD, function(i) c(0,diff(i))), .SDcols=var_cols]\nsplits_ <- train_val_test_split(dt[,-1], train_size = 0.5)\ndf_train <- splits_$train\ndf_val <- splits_$val\ndf_test <- splits_$test\n\nRecall that the neural network that we are aiming to build takes as its input its own \\(p\\) lags as well as \\(p\\) lags of all other variables in the system.\n\nlibrary(torch)\n\ndvar_dataset <- dataset(\n  name = \"dvar_dataset\",\n  \n  initialize = function(X, response, lags, n_ahead, sample_frac = 1) {\n    \n    self$lags <- lags\n    self$n_ahead <- n_ahead\n    self$response <- response\n    self$train_mean <- colMeans(X)\n    self$train_sd <- sapply(1:ncol(X), function(i) sd(X[,i]))\n    self$X <- torch_tensor(t((t(X) - self$train_mean)/self$train_sd)) # of dimension (D x T)\n    \n    n <- dim(self$X)[1] - self$lags - self$n_ahead + 1\n    \n    self$starts <- sort(sample.int(\n      n = n,\n      size = round(n * sample_frac)\n    ))\n    \n  },\n  \n  .getitem = function(i) {\n    \n    start <- self$starts[i]\n    end <- start + self$lags - 1\n    pred_length <- self$n_ahead\n    \n    list(\n      X = self$X[start:end,],\n      y = self$X[(end + 1):(end + pred_length),self$response]\n    )\n    \n  },\n  \n  .length = function() {\n    length(self$starts) \n  }\n)\n\n\nset.seed(123)\nresponse_var_idx <- 1\nlags <- 6\nn_ahead <- 12\ntrain_ds <- dvar_dataset(df_train, response_var_idx, lags, n_ahead = n_ahead, sample_frac = 0.5)\n\nbatch_size <- 30\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\n\nvalid_ds <- dvar_dataset(df_val, response_var_idx, lags, n_ahead=n_ahead, sample_frac = 0.5)\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n\ntest_ds <- dvar_dataset(df_test, response_var_idx, lags, n_ahead=n_ahead)\ntest_dl <- test_ds %>% dataloader(batch_size = 1)\n\nLetâ€™s do a quick sanity check to see if the dimensions check out:\n\ntrain_ds[1]\n\n$X\ntorch_tensor\n 0.1020 -1.1904  0.9092 -0.4763\n 0.9909  0.5094  1.8195 -0.4147\n 1.1777  1.1057  1.2633  0.2022\n 0.6616 -0.9201  1.6707 -0.4969\n-0.6957 -2.2618 -0.5863  0.5517\n-1.3460 -1.2138  3.2111  1.9088\n[ CPUFloatType{6,4} ]\n\n$y\ntorch_tensor\n-2.0908\n-2.8897\n-2.9660\n-1.4641\n 0.0895\n 1.1206\n 1.4424\n 0.0236\n-0.4609\n 0.1643\n 0.6791\n-0.0035\n[ CPUFloatType{12} ]\n\n\nThis looks like what we expected: X is a \\((D \\times p)\\) tensor and \\(y\\) is just the single output.\n\nin_idx <- train_ds$starts[1]:(train_ds$starts[1]+train_ds$lags-1)\nout_idx <- c(max(in_idx)+1,response_var_idx)\nX_train <- t((t(df_train) - colMeans(df_train)) / sapply(1:ncol(df_train), function(i) sd(df_train[,i])))\nX_train[in_idx,]\n\n              e       prod         rw          U\n[1,]  0.1019674 -1.1904140  0.9092028 -0.4763473\n[2,]  0.9908521  0.5093805  1.8195315 -0.4146620\n[3,]  1.1776523  1.1057034  1.2633140  0.2021906\n[4,]  0.6615854 -0.9200533  1.6706661 -0.4969091\n[5,] -0.6956522 -2.2618121 -0.5862564  0.5517404\n[6,] -1.3459703 -1.2137674  3.2111416  1.9088162\n\nX_train[out_idx[1]:(out_idx[1]+n_ahead-1),out_idx[2]]\n\n [1] -2.090789787 -2.889711226 -2.966011884 -1.464126631  0.089478145\n [6]  1.120601201  1.442417457  0.023572063 -0.460867622  0.164265269\n[11]  0.679070086 -0.003545797\n\n\nWhat about the mini-batch? We see that X is of the desired dimension (batch_size, n_timesteps, num_features).\n\nlength(train_dl)\n\n[1] 1\n\nb <- train_dl %>% dataloader_make_iter() %>% dataloader_next()\nb\n\n$X\ntorch_tensor\n(1,.,.) = \n -2.0908  0.1093  1.7242  1.0247\n -2.8897 -1.1323  0.8589  3.2865\n -2.9660  0.4020  0.2097  3.6360\n -1.4641 -0.1386  0.6799  1.1686\n  0.0895  1.6895 -1.6889 -0.7025\n  1.1206  0.4061 -0.2826 -0.4147\n\n(2,.,.) = \n  0.1020 -1.1904  0.9092 -0.4763\n  0.9909  0.5094  1.8195 -0.4147\n  1.1777  1.1057  1.2633  0.2022\n  0.6616 -0.9201  1.6707 -0.4969\n -0.6957 -2.2618 -0.5863  0.5517\n -1.3460 -1.2138  3.2111  1.9088\n\n(3,.,.) = \n -2.8897 -1.1323  0.8589  3.2865\n -2.9660  0.4020  0.2097  3.6360\n -1.4641 -0.1386  0.6799  1.1686\n  0.0895  1.6895 -1.6889 -0.7025\n  1.1206  0.4061 -0.2826 -0.4147\n  1.4424 -0.1345 -0.7919 -1.0932\n\n(4,.,.) = \n  1.1777  1.1057  1.2633  0.2022\n  0.6616 -0.9201  1.6707 -0.4969\n -0.6957 -2.2618 -0.5863  0.5517\n -1.3460 -1.2138  3.2111  1.9088\n -2.0908  0.1093  1.7242  1.0247\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{12,6,4} ]\n\n$y\ntorch_tensor\nColumns 1 to 10 1.4424  0.0236 -0.4609  0.1643  0.6791 -0.0035 -0.0456  0.8483  0.6847  0.6916\n-2.0908 -2.8897 -2.9660 -1.4641  0.0895  1.1206  1.4424  0.0236 -0.4609  0.1643\n 0.0236 -0.4609  0.1643  0.6791 -0.0035 -0.0456  0.8483  0.6847  0.6916  0.8213\n-2.9660 -1.4641  0.0895  1.1206  1.4424  0.0236 -0.4609  0.1643  0.6791 -0.0035\n 0.8213  0.2048 -0.3137 -0.0030  0.1853  1.1057  0.6218  1.1774  0.5537  0.0394\n 0.6916  0.8213  0.2048 -0.3137 -0.0030  0.1853  1.1057  0.6218  1.1774  0.5537\n-0.4609  0.1643  0.6791 -0.0035 -0.0456  0.8483  0.6847  0.6916  0.8213  0.2048\n 0.1853  1.1057  0.6218  1.1774  0.5537  0.0394 -0.2996  0.4384  0.9216 -0.6563\n-0.0035 -0.0456  0.8483  0.6847  0.6916  0.8213  0.2048 -0.3137 -0.0030  0.1853\n-0.0456  0.8483  0.6847  0.6916  0.8213  0.2048 -0.3137 -0.0030  0.1853  1.1057\n-0.3137 -0.0030  0.1853  1.1057  0.6218  1.1774  0.5537  0.0394 -0.2996  0.4384\n-1.4641  0.0895  1.1206  1.4424  0.0236 -0.4609  0.1643  0.6791 -0.0035 -0.0456\n\nColumns 11 to 12 0.8213  0.2048\n 0.6791 -0.0035\n 0.2048 -0.3137\n-0.0456  0.8483\n-0.2996  0.4384\n 0.0394 -0.2996\n-0.3137 -0.0030\n-0.1092 -0.0666\n 1.1057  0.6218\n 0.6218  1.1774\n 0.9216 -0.6563\n 0.8483  0.6847\n[ CPUFloatType{12,12} ]"
  },
  {
    "objectID": "rnn_torch.html#model",
    "href": "rnn_torch.html#model",
    "title": "RNN in Torch",
    "section": "Model",
    "text": "Model\n\nmodel <- nn_module(\n  \n  initialize = function(type=\"lstm\", input_size, hidden_size, linear_size, output_size,\n                        num_layers = 2, dropout = 0.25, linear_dropout = 0.25) {\n    \n    self$type <- type\n    self$num_layers <- num_layers\n    self$linear_dropout <- linear_dropout\n    \n    self$rnn <- if (self$type == \"gru\") {\n      nn_gru(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    } else {\n      nn_lstm(\n        input_size = input_size,\n        hidden_size = hidden_size,\n        num_layers = num_layers,\n        dropout = dropout,\n        batch_first = TRUE\n      )\n    }\n    \n    self$mlp <- nn_sequential(\n      nn_linear(hidden_size, linear_size),\n      nn_relu(),\n      nn_dropout(linear_dropout),\n      nn_linear(linear_size, output_size)\n    )\n    \n  },\n  \n  forward = function(x) {\n    \n    x <- self$rnn(x)\n    x[[1]][ ,-1, ..] %>% \n      self$mlp()\n    \n  }\n  \n)\n\n\nD <- dim(train_ds[1]$X)[2]\nnet <- model(input_size = D, hidden_size = 32, linear_size = 128, output_size = n_ahead)\ndevice <- torch_device(if (cuda_is_available()) \"cuda\" else \"cpu\")\nnet <- net$to(device = device)"
  },
  {
    "objectID": "rnn_torch.html#training",
    "href": "rnn_torch.html#training",
    "title": "RNN in Torch",
    "section": "Training",
    "text": "Training\n\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 30\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad() # in\n  output <- net(b$X$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$backward()\n  optimizer$step()\n  \n  loss$item()\n}\n\nvalid_batch <- function(b) {\n  \n  output <- net(b$X$to(device = device))\n  target <- b$y$to(device = device)\n  \n  loss <- nnf_mse_loss(output, target)\n  loss$item()\n  \n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  train_loss <- c()\n  \n  coro::loop(for (b in train_dl) {\n    loss <- train_batch(b)\n    train_loss <- c(train_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, training: loss: %3.5f \\n\", epoch, mean(train_loss)))\n  \n  net$eval()\n  valid_loss <- c()\n  \n  coro::loop(for (b in valid_dl) {\n    loss <- valid_batch(b)\n    valid_loss <- c(valid_loss, loss)\n  })\n  \n  cat(sprintf(\"\\nEpoch %d, validation: loss: %3.5f \\n\", epoch, mean(valid_loss)))\n}\n\n\nEpoch 1, training: loss: 0.62297 \n\nEpoch 1, validation: loss: 0.79906 \n\nEpoch 2, training: loss: 0.61487 \n\nEpoch 2, validation: loss: 0.78826 \n\nEpoch 3, training: loss: 0.60945 \n\nEpoch 3, validation: loss: 0.77741 \n\nEpoch 4, training: loss: 0.60109 \n\nEpoch 4, validation: loss: 0.76667 \n\nEpoch 5, training: loss: 0.60198 \n\nEpoch 5, validation: loss: 0.75596 \n\nEpoch 6, training: loss: 0.59502 \n\nEpoch 6, validation: loss: 0.74520 \n\nEpoch 7, training: loss: 0.58521 \n\nEpoch 7, validation: loss: 0.73451 \n\nEpoch 8, training: loss: 0.57680 \n\nEpoch 8, validation: loss: 0.72386 \n\nEpoch 9, training: loss: 0.57761 \n\nEpoch 9, validation: loss: 0.71318 \n\nEpoch 10, training: loss: 0.57223 \n\nEpoch 10, validation: loss: 0.70240 \n\nEpoch 11, training: loss: 0.56296 \n\nEpoch 11, validation: loss: 0.69152 \n\nEpoch 12, training: loss: 0.55892 \n\nEpoch 12, validation: loss: 0.68051 \n\nEpoch 13, training: loss: 0.55917 \n\nEpoch 13, validation: loss: 0.66926 \n\nEpoch 14, training: loss: 0.54466 \n\nEpoch 14, validation: loss: 0.65784 \n\nEpoch 15, training: loss: 0.54911 \n\nEpoch 15, validation: loss: 0.64616 \n\nEpoch 16, training: loss: 0.54496 \n\nEpoch 16, validation: loss: 0.63420 \n\nEpoch 17, training: loss: 0.54140 \n\nEpoch 17, validation: loss: 0.62209 \n\nEpoch 18, training: loss: 0.53267 \n\nEpoch 18, validation: loss: 0.60992 \n\nEpoch 19, training: loss: 0.52077 \n\nEpoch 19, validation: loss: 0.59760 \n\nEpoch 20, training: loss: 0.52184 \n\nEpoch 20, validation: loss: 0.58506 \n\nEpoch 21, training: loss: 0.52193 \n\nEpoch 21, validation: loss: 0.57236 \n\nEpoch 22, training: loss: 0.51467 \n\nEpoch 22, validation: loss: 0.55957 \n\nEpoch 23, training: loss: 0.50776 \n\nEpoch 23, validation: loss: 0.54668 \n\nEpoch 24, training: loss: 0.50670 \n\nEpoch 24, validation: loss: 0.53390 \n\nEpoch 25, training: loss: 0.49992 \n\nEpoch 25, validation: loss: 0.52132 \n\nEpoch 26, training: loss: 0.50380 \n\nEpoch 26, validation: loss: 0.50928 \n\nEpoch 27, training: loss: 0.49770 \n\nEpoch 27, validation: loss: 0.49789 \n\nEpoch 28, training: loss: 0.48669 \n\nEpoch 28, validation: loss: 0.48728 \n\nEpoch 29, training: loss: 0.48587 \n\nEpoch 29, validation: loss: 0.47759 \n\nEpoch 30, training: loss: 0.49154 \n\nEpoch 30, validation: loss: 0.46938"
  },
  {
    "objectID": "rnn_torch.html#evaluation",
    "href": "rnn_torch.html#evaluation",
    "title": "RNN in Torch",
    "section": "Evaluation",
    "text": "Evaluation\nBelow we see the 12-step prediction\n\nnet$eval()\n\ntest_preds <- vector(mode = \"list\", length = length(test_dl))\n\ni <- 1\n\ncoro::loop(for (b in test_dl) {\n  \n  input <- b$X\n  output <- net(input$to(device = device))\n  preds <- as.numeric(output)\n  \n  test_preds[[i]] <- preds\n  i <<- i + 1\n  \n})\n\ny_hat <- test_preds[[1]] * train_ds$train_sd[response_var_idx] + train_ds$train_mean[response_var_idx]\ny_hat <- data.table(y_hat)[,id:=1:.N][,variable:=colnames(df_train)[response_var_idx]]\nsetkey(y_hat, id, variable)\n\nlibrary(data.table)\ny_true <- data.table(rbind(df_val,df_test))\ny_true <- y_true[,..response_var_idx]\ny_true[,id:=(-nrow(df_val)-lags+1):(-nrow(df_val)-lags+y_true[,.N])]\ny_true <- melt(y_true, id.vars = \"id\", value.name = \"y\")\nsetkey(y_true, id, variable)\n\ndt_plot <- melt(y_hat[y_true],id.vars = c(\"id\",\"variable\"), variable.name = \"type\")\ndt_plot[type==\"y_hat\" & id==0, value:=dt_plot[type==\"y\" & id==0]$value]\nlibrary(ggplot2)\n\nggplot(dt_plot[id<=n_ahead], aes(x=id, y=value, colour=type)) +\n  geom_line() +\n  geom_point() +\n  scale_color_manual(name=\"Type:\", values=c(\"blue\", \"red\"), labels=c(\"Prediction\", \"Actual\")) +\n  labs(\n    title = sprintf(\"Variable: %s\", colnames(df_train)[response_var_idx]),\n    x = \"Time\",\n    y = \"Value\"\n  )"
  }
]